Audit of Olivine-V11: Transition to File-Centric Knowledge Clusters

File Ingestion System – From Monolithic Files to Cluster Entry Points

Current State: The file ingestion pipeline (managed by the File Steward agent) treats each file largely as a single entity. On a file event, the system upserts a File node in the graph, classifies it into a Canonical Slot (a categorical placeholder like “SCRIPT_PRIMARY” or “BUDGET_MASTER”), updates folder hierarchy links, and records an EdgeFact for the classification ￼ ￼. The File node stores metadata (size, path, etc.) and even raw extracted text (in extractedText) for search ￼. This approach treats the file as a monolithic container of content, with at most one classification and a blob of text rather than structured knowledge.

Needed Changes: To align with the “cluster, not container” paradigm, file ingestion must evolve to instantiate a cluster of nodes and relationships per file rather than a lone node:
	•	Structured Content Extraction: After the initial file node upsert and classification, introduce steps to parse and extract the file’s content into Idea-layer nodes. For example, when a script PDF is ingested, automatically create Scene and Character content nodes for each scene and character in the script. These should be represented as separate nodes (e.g. as generic Content nodes with contentType="Scene" or as specific Scene nodes) linked to the File ￼. Likewise, a call sheet file could yield ShootDay (IRL layer) and associated scheduling nodes, and a budget spreadsheet could produce Budget and cost breakdown nodes (Ops layer).
	•	Link Files to Generated Content Nodes: Use the graph to link the file node to all content it contains or relates to. The schema already anticipates this with relationships like File.content and Content.references ￼. Leverage these: for each extracted piece of content, create a Content node (or appropriate typed node) and add a REFERENCES/DERIVED_FROM relationship back to the source File. This makes the File an entry point into a cluster of idea/ops nodes rather than an isolated endpoint. For instance, a shooting script file would reference dozens of Scene content nodes, which in turn could link to Character nodes, etc.
	•	Multi-Slot Classification: If applicable, allow a file to fill multiple canonical slots or categories when appropriate (cluster perspective). The current system assigns one slot via an EdgeFact (temporal classification) ￼. In a cluster model, a single file might be relevant to multiple slots (e.g. a PDF that contains both schedule and budget info). Updating the classification mechanism to support multiple slot assignments or a more granular tagging of file content ensures the file is not pigeonholed as one thing when it spans domains. This might involve creating multiple CLASSIFIED_AS EdgeFacts (one per slot) or subdividing the file into sub-documents each classified separately.
	•	Decouple Raw Text from File Node: The extractedText field on the File ￼should be reconsidered. Instead of storing entire text in the File node (monolithic design), use that text to populate content nodes (which capture structured knowledge) or store it in an external index for search. The File node can link to a Content node that holds structured data (or to many content nodes), rather than holding unstructured text itself. This change moves the system away from treating the file as the container of all its data and toward treating it as a gateway into the graph of data.

Actionable Recommendations:
	•	Extend the FileStewardAgent workflow to include a content parsing phase after classification. For each file type (script, call sheet, budget, etc.), either invoke a specialized parser or a dedicated agent to produce graph nodes (scenes, schedules, line items, etc.) from the file’s contents. Ensure this happens within a provenance-tracked commit context (so creation of these nodes is logged in the Provenance layer).
	•	Implement mappings from canonical slots to parser logic. For example, if a file is classified as SCRIPT_PRIMARY, trigger the script-to-graph conversion routine. If classified as CALLSHEET_FINAL, trigger a call sheet parser. This could be done via the Agent Orchestrator dispatching a new job to a “ContentExtractor” agent or by FileStewardAgent itself calling the appropriate tool.
	•	Modify the data model so that new content nodes are properly linked. Use the existing File.content relationship to attach extracted content to the file node ￼ (or create new relationship types if needed for clarity, e.g. CONTAINS_SCENE). For each relationship created, consider using EdgeFacts if temporal tracking is needed (for example, an EdgeFact of type CONTAINS with valid time, if we want to track when a file gained a particular content element).
	•	Remove or minimize the direct use of File.extractedText. Instead, store necessary textual data in content nodes (e.g. scene dialogue as a property of a Scene node) or an indexed search service. This ensures the file node remains an entry point with links to knowledge, rather than the sole holder of that knowledge.

By implementing these changes, each ingested file will spawn a network of nodes across IRL/Idea/Ops layers, firmly establishing it as an entry point to a knowledge cluster rather than a dead-end container of data.

Agent Architecture – Task Orchestration & Ontology Traversal

Current State: The agent system is composed of specialized agents with distinct roles (File Steward, Ontology Curator, Call Sheet Composer, Budget Composer, Compliance Agent, etc.), coordinated by an Agent Orchestrator/Router ￼ ￼. At present, each agent tends to operate within a single ontology layer or functional domain: e.g. File Steward maintains the File ontology (storage layer) and handles file classification, while the Composer agents generate documents by pulling data from the graph. Agents execute tasks largely in isolation, and the orchestration logic routes tasks to one agent at a time based on type. The notion of autonomy is present (some agents can trigger proposals or automated actions, e.g. the OntologyCurator auto-approving schema changes ￼), but agents mostly follow predefined workflows. Critically, because files were treated as monolithic, agents haven’t needed to extensively traverse across ontology layers in a single job – their operations were siloed (e.g., classify a file, or compose a document from existing data).

Needed Changes: Embracing the cluster model means agents must be capable of traversing and updating multiple ontology layers in the course of a task. Task orchestration should support multi-step, cross-domain workflows, and agents might need new responsibilities or collaboration patterns:
	•	Cross-Ontology Traversal: Agents should leverage the graph’s relationships to move between IRL, Idea, and Ops layers during reasoning. For example, a CallSheetComposer agent generating a shoot-day call sheet should navigate from an IRL ShootDay node to related Idea layer nodes (Scenes scheduled that day) and IRL layer nodes (Talents/Crew assigned), and possibly to Ops nodes (equipment bookings, etc.). If the data is missing, the agent might flag it or even invoke other agents to supply it. Currently, if these links are not well-modeled or if the agent only knows to look in one place, this traversal must be enabled by both schema and agent logic.
	•	Task Decomposition & Sequencing: Introduce the ability for complex tasks to be broken into sub-tasks that different agents handle in sequence or in parallel. Rather than one agent trying to do everything or working blind, the Orchestrator can assign a pipeline: e.g., on a new script file ingest, the File Steward agent handles basic ingestion, then a ScriptParser agent (new) handles content extraction (Idea layer population), then perhaps a Scheduler agent cross-references the new scenes with existing schedules (IRL/Ops), and finally a Composer agent might update a document. This orchestration across agents needs to be formally defined. It could be rule-based (if file type X then invoke agents A -> B -> C) or dynamic based on graph state (if cluster has unmet needs, spawn an agent to fill them).
	•	Enhanced Autonomy and Triggering: Allow agents to proactively trigger workflows when cluster inconsistencies or updates are detected. For instance, if an agent (or a background watcher) notices that a Scene node exists without a linked ShootDay (Idea content not scheduled in IRL layer), it could autonomously create a task for a scheduling agent or raise an alert. This requires agents to have some awareness beyond their narrow scope – effectively a form of inter-agent communication via the graph or a messaging system.
	•	Ontology-Aware Reasoning: Equip agents with knowledge of the ontology structure so they can exploit it. For example, File Steward currently just classifies and applies taxonomy rules within the File layer ￼ ￼. Going forward, File Steward (or its successors) should understand that after classifying a file as a script, the next logical step is to populate the Idea ontology (scenes, characters). This might involve the agent consulting the ontology schema (perhaps via the Ontology Curator agent) to determine what kinds of nodes to create. In general, agents should use the ontology meta-knowledge (the fact that a CanonicalSlot “SCRIPT_PRIMARY” corresponds to certain Idea-layer entities) as part of their decision-making.
	•	Agent Role Revisions: We may need to redefine some agent roles or add new ones to cover the cluster maintenance lifecycle. The current roster has a File Steward (for ingestion), then jumps to high-level composers and a curator. Possibly introduce a Content Curator or Extractor agent that focuses on keeping the Idea layer in sync with files (extracting or updating scenes, etc.), and a Scheduler agent that ensures Ops (schedules, budgets) align with Idea (script, plans) and IRL (actual production events). These agents would naturally traverse multiple layers as part of their job. The existing OntologyCurator might also need to broaden scope to ensure that any new node types or relationships needed for cross-layer links are proposed and approved (governance for the evolving cluster schema).

Actionable Recommendations:
	•	Develop workflow definitions for multi-agent orchestration. For each major file type or cluster scenario, map out which agents need to run and in what sequence. Implement this in the Agent Orchestrator. This might involve adding a job queue that can chain tasks (e.g., a file ingestion job on the queue, once completed, automatically enqueues a content-extraction job, etc., possibly using the commit events as triggers).
	•	Enhance the Agent Router logic to route tasks not just by single-agent responsibility but by cluster context. For example, allow an agent to produce an intermediate result or event (e.g. “Scenes extracted for Script X”) that the router uses to dispatch follow-up tasks to another agent. This could be facilitated by standardizing such events in the Provenance layer (each agent’s actions are already recorded as Action entries tied to a commit ￼; the system can look at these and decide next steps).
	•	Invest in graph queries within agents to utilize the cluster. Agents should frequently query the Neo4j knowledge graph to gather all related nodes for a task. For instance, before a ComplianceAgent checks a document, it should pull not just the file, but all its linked content nodes and context (e.g., find all compliance rules (Ops) that apply to the types of content in this file’s cluster). This means writing or optimizing query helpers for agents so they can easily get “the cluster around File X” – perhaps by traversing out from the file node across all relationship types within N hops.
	•	Testing & Iteration: Simulate a few end-to-end scenarios (ingest a script and produce a cluster, change a scene and propagate to schedule, etc.) in a staging environment. Identify where agents fail to hand off or where data isn’t updated across layers, and adjust the orchestration rules or agent logic accordingly. This will likely reveal specific needed improvements (e.g., a scene added to a script might not automatically create a new shoot day – the fix might be to have the system open a “proposal” or task for a producer user or scheduling agent).

By redesigning the agent workflows in this manner, each agent becomes a steward of part of the cluster, and collectively they maintain the cluster’s consistency. Agents will operate less in silos and more as a collaborative swarm navigating the connected graph, which is essential for the new multi-layer paradigm.

Ontology Schema Design – Supporting “Cluster, Not Container”

Current State: The Olivine V11 ontology is conceptually divided into layers (though not always explicitly named as IRL/Idea/Ops in code). We see evidence of a Content (Idea) layer – e.g. Scene, Character, etc. – and an Operations layer – e.g. Budget, PurchaseOrder, ComplianceRule – defined in the GraphQL schema ￼ ￼. The File (IRL storage) layer is defined with Source, Folder, File, etc., and files are linked to a CanonicalSlot classification ￼ ￼. The Provenance layer is implemented via commits, actions, versions, and edge facts ￼ ￼. This is a solid foundation: the graph data model can capture various entity types and temporal relationships. However, to fully realize “files as entry points to node clusters,” the schema needs to ensure that each file can connect to a rich subgraph of Idea/IRL/Ops nodes. Currently, some linkages exist (file to canonical slot, file to project, file to content via references) but many are planned or partial. For example, the Content node type exists to hold extracted content pieces and has a relationship for referencing files ￼, but the actual use of this in ingestion may be incomplete. Likewise, Ops entities like PurchaseOrder carry only textual references to Idea layer (a scene_id string and crew_role text) rather than explicit relationships to Scene or Crew nodes ￼, implying the schema or its usage is not fully linking those layers yet.

Needed Changes: The ontology schema should be reviewed and extended to ensure it natively supports the cluster concept – meaning any concept that logically links across IRL, Idea, and Ops is represented explicitly as a node or relationship. Key improvements include:
	•	Explicit Cross-Layer Relationships: Introduce relationship types (or node types) that connect the layers. For example, define a relationship PORTRAYED_BY between an Idea-layer Character and an IRL-layer Talent (actor) to capture casting. Define a relationship SCHEDULED_ON between an Idea-layer Scene and an IRL ShootDay to represent scheduling of scenes on real production dates. These links likely exist implicitly in the domain but may not be in the model yet – making them first-class citizens in the ontology will break down silos. (If such links are meant to be captured as Content nodes or via EdgeFacts, ensure those mechanisms are in place. For instance, a Content node of type “SceneScheduling” could connect a Scene and a ShootDay, but a direct relationship is simpler.)
	•	Layer Identifier or Segregation: Consider tagging or partitioning nodes by ontology layer (e.g., labels or a property like layer: "IRL" on Talent, Crew, ShootDay, versus layer: "Idea" on Scene, Character, etc.). This isn’t strictly necessary, but it can enforce clarity that each node belongs to one layer and should only have certain types of relationships to others. It can also help agents or queries restrict scope when needed (e.g., find all IRL entities related to a given Idea entity). The code already groups types (the GraphQL schema’s sections imply what’s considered content vs operations), but an explicit ontology mapping could aid maintenance. The OntologyCurator agent could enforce or suggest these layer distinctions when new types are added.
	•	Support for Hierarchical Clusters: Ensure the schema can represent not just flat links, but grouped clusters of nodes. For instance, a Content cluster extracted from a file might need an organizing node (like a “Document” or ContentSet node) that groups scenes or breakdown elements derived from one file. In the current model, all content nodes can reference the file directly ￼, which is fine, but adding a grouping could be useful if a file logically contains sections. Another approach is simply using the file node as the hub (which is the entry point concept). The schema already allows multiple content nodes to reference one file, achieving a star-shaped cluster around the file. This is likely sufficient, but keep an eye on whether more complex cluster topologies (content nodes linking to each other in a hierarchy) need to be formalized. For example, a script might be broken into acts (Act nodes containing Scenes). If needed, add those as intermediate nodes or let Content nodes reference other Content nodes (which the derivedFrom relationship already supports ￼).
	•	Revisit Use of Generic Content vs Specific Types: The presence of a generic Content type with a contentType field and the simultaneous presence of specific types like Scene, Character in the GraphQL API is a bit inconsistent. It might be that internally all content pieces are stored as Content nodes with type descriptors, and the specific GraphQL types are projections. If so, ensure this approach is robust for cluster use: e.g., a Content node with contentType="Scene" should have properties or linked nodes for details like scene number, etc. (In GraphQL, the Scene type is defined with specific fields like number, title, etc. ￼, which suggests either those are separate nodes or the system uses polymorphism.) It might be clearer to fully adopt an ontology where Scenes, Characters, ShootDays, etc., are distinct labels (classes) in the graph and in the API, which aligns with human understanding. This would make the cluster clearly composed of heterogeneous node types. If the current plan is to use the generic Content node for everything, then ensure that the metadata or other fields on Content can carry all needed info, or pivot to distinct types for clarity. Action: Decide on one approach and refactor accordingly (this is a design clarification to avoid confusion in cluster composition).
	•	Temporal/Version Nodes for Cross-Layer Links: Since the Provenance layer uses EdgeFact nodes to represent time-bounded relationships (as seen for file classification ￼), consider using EdgeFacts for cross-layer relationships that may change over time. For example, a Scene might initially be scheduled on ShootDay 5, but later moved to ShootDay 6. Modeling SCHEDULED_ON as an EdgeFact (with from_id = sceneId, to_id = shootDayId, valid_from, valid_to) would allow tracking such changes over time in the cluster ￼ ￼. The current schema has an EdgeFact type defined ￼ and even GraphQL mutations for creating and ending EdgeFacts ￼, indicating the infrastructure is ready. Use it for relationships that benefit from history (schedule changes, recasting of a role, budget line item adjustments, etc.). This will tightly integrate the Provenance layer with IRL/Idea/Ops layers in the cluster.

Actionable Recommendations:
	•	Schema Audit & Extension: Perform a thorough review of the existing ontology schema definitions (both in code and docs) to enumerate all missing or weak connections between layers. For each, decide whether to add a direct relationship or an intermediate node. Concretely, implement links such as:
	•	Character -[:PORTRAYED_BY]-> Talent (Idea to IRL link)
	•	Scene -[:SHOOT_DAY]-> ShootDay or ShootDay -[:CONTAINS_SCENE]-> Scene (Idea to IRL link)
	•	Crew -[:ASSIGNED_TO]-> ShootDay (IRL link, or via call sheet content)
	•	Scene -[:BUDGETED_BY]-> Budget or linking scenes to specific budget elements (Idea to Ops link) if granular budgets per scene are tracked
	•	Ensure PurchaseOrder.scene_id and crew_role (currently strings) are replaced or supplemented by proper relationships to Scene and Crew nodes ￼ in the graph model. For example, a PurchaseOrder -[:FOR_SCENE]-> Scene and PurchaseOrder -[:FOR_CREW_ROLE]-> Crew (or directly to a Crew member if known).
	•	Leverage EdgeFacts for Key Relationships: Identify relationships that represent facts that can change or be invalidated (like “X is scheduled for Y date”) and convert them to use EdgeFact nodes with valid_from/valid_to. Update the mutation/creation logic to create such EdgeFacts instead of plain relationships. This aligns with the existing provenance approach and ensures historical integrity within clusters ￼ ￼.
	•	Update Documentation: Clearly document the four ontology layers (IRL, Idea, Ops, Provenance) in the code or markdown docs (e.g., an updated Section 1 of the Blueprint). Enumerate which node labels belong to which layer and what cross-layer relationships exist. This will help developers and the OntologyCurator agent maintain the model. Given that the File Ontology doc explicitly mentions connecting to content and ops ontologies ￼, similar documentation for the Idea and Ops ontologies should be written/updated to reflect the “cluster” philosophy (e.g., the Idea ontology doc should describe how it links to IRL and Ops).
	•	Migration Plan: For any schema changes (new relationships or node types), plan a migration of existing data. For example, if you replace scene_id text in PurchaseOrder with a real relationship, write a script or migration to retroactively link any existing POs to Scene nodes (if Scenes exist). Likewise, if you create Character–Talent links, ensure any known mappings are created. This will bootstrap the clusters for existing projects.

In summary, the schema should no longer treat a file or any artifact as an island. It must facilitate a richly interlinked set of ontological layers, so that when traversed, one can start at a File and easily find its conceptual content, the real-world elements associated with that content, the operational records tied to it, and the provenance of all those links. After these updates, the knowledge graph will truly reflect a cluster per file, enabling agents to maintain and reason over the interconnected data.

Agent-to-Agent Interfaces & Orchestration Models

Current State: The coordination between agents in V11 is primarily handled by the central Orchestrator/Router, which assigns tasks to the appropriate agent based on task type ￼. Agents do not currently call each other directly; there isn’t an exposed interface where one agent invokes another agent’s functions. Instead, communication is indirect (e.g., via the Task Queue or shared database state). Each agent’s actions are logged as commit actions, but one agent does not explicitly signal another except through the orchestrator. This means the “interfaces” between agents are fairly limited – effectively the interface is the shared knowledge graph and the task queue. The orchestration model is thus a hub-and-spoke (central dispatcher to agents). While this prevents chaotic interactions, it can also lead to sequential processing and potential bottlenecks if one agent’s output needs further handling (someone has to notice and queue the next task).

Needed Changes: As we shift to cluster-centric processing, a more fluid interplay between agents is beneficial. Agents should be able to cooperate on the same cluster, either through orchestrator-facilitated chaining or through direct signals. We need to design clear interfaces for this cooperation:
	•	Shared Context via Clusters: Define a standard representation of “context” that agents can pass along. For instance, if a FileStewardAgent finishes processing a file and creating initial nodes, it could attach a reference to the newly created cluster (like a cluster ID or the File ID) in a task result or event. The next agent (say, ContentExtractorAgent) picks up that context and knows which cluster (which file and related nodes) to operate on. This could be implemented by embedding the file’s ID (or project + file ID) in the task message or by having agents write to a “pending tasks” node in the graph that others read. The key is to standardize how agents refer to clusters and partial outcomes, so that one agent’s output seamlessly becomes another’s input.
	•	Event-Driven Triggers: Augment the orchestrator with event-driven logic. For example, when the FileStewardAgent logs a successful classification and content extraction commit, the system could automatically trigger a follow-up action (similar to how a CI pipeline triggers stages). This might be accomplished by subscribing to certain events (commit of type X or new nodes of type Y created) and having handler code that enqueues tasks for the appropriate next agent. In effect, use the Provenance layer as an interface – the commit log becomes a message bus. For instance, a commit that adds new Scene nodes could implicitly “call” the Scheduler agent to integrate those scenes into the shooting schedule, because the system knows that whenever new scenes appear, scheduling is needed.
	•	Direct Agent Calls (if needed): We might allow agents to explicitly invoke others through a defined API. This could be as simple as one agent creating a task entry that is addressed to another agent. For example, the ContentExtractorAgent, after parsing a script, could create a task “UpdateSchedule” for the ScheduleComposerAgent. The orchestrator would see this and route it. This is still mediated by the orchestrator, but the key is the interface: a common data format for one agent to request work from another. We should design a minimal agent job interface – e.g., all agents accept inputs like { clusterId, actionType, payload }. In code, this could be a method on each agent or a message structure in the queue. Document these interfaces so each agent knows how to formulate a request to another.
	•	Agent Autonomy vs. Orchestrator Control: Determine the balance between agents acting on their own versus via the orchestrator. To maintain clarity and avoid infinite loops, it’s safer to keep the orchestrator as the coordinator that “sees the big picture.” However, the orchestrator’s logic can incorporate rules that effectively encode agent-to-agent contracts. For example, orchestrator logic: “If FileSteward completes a file of type SCRIPT_PRIMARY, queue a ContentExtraction job; once that completes, queue a ScheduleUpdate job,” etc. This is an orchestration model update – moving from one-step dispatch to multi-step workflows as described earlier. Ensure the orchestrator has access to any context it needs (which means agents must report sufficient info on completion, e.g., “file X processed, produced Y scenes”).
	•	Isolation and Error Handling: When introducing multi-agent workflows, clearly define what happens on failure at each stage. The interfaces should include feedback channels – e.g., if the Schedule agent finds that some Scenes have no corresponding ShootDay, it might either create a placeholder or return an error that triggers a different path (maybe escalate to a human or to the OntologyCurator if a new concept is needed). The orchestration model should catch these and decide next steps (retry, skip, alert). Essentially, treat the chain of agents as a transaction that can roll back or compensate if something goes wrong in the cluster assembly.

Actionable Recommendations:
	•	Implement an Event Bus or Pub-Sub on Commits: Utilize the existing commit/action logging in Neo4j or an external pub-sub (if available) to broadcast events like “File X ingested” or “Scene Y created.” Build lightweight listeners (which could even be part of the Agent Orchestrator process) that respond by enqueuing tasks for downstream agents. This decouples direct calls – an agent just commits its work and doesn’t need to know who picks it up, while downstream logic reacts to the event. Start with key events (file processed, content extracted, etc.) and expand as needed.
	•	Enhance the Task Queue Schema: Currently, tasks are likely just labeled by agent type and contain minimal info. Expand the task payload to include references to the relevant graph nodes (IDs of file or content). For instance, a task for the BudgetComposer might include projectId or a list of scene IDs it should budget for. Define what each agent expects in its task input and have preceding steps provide that. Document these in the code (e.g., as TypeScript interfaces or in the docs).
	•	Inter-agent Data Contracts: For each pair of agents that interact in a workflow, specify the contract. Example: ContentExtractorAgent -> ScheduleAgent: “expects all new Scene nodes to have a projectId and a status (e.g., ‘unscheduled’), and the ScheduleAgent will change status to ‘scheduled’ and link them to ShootDays.” These contracts ensure that the first agent produces data the second can use. If the first agent can’t fully meet the contract (maybe missing info), decide whether the second agent will fill it (maybe ScheduleAgent creates a new ShootDay because none existed). Being explicit here will surface any oversight in how separate ontologies connect in practice.
	•	Testing Multi-Agent Orchestration: Create a test scenario in which a new file goes through several agents. Observe the hand-off. For example, ingest a script -> expect FileSteward to output file + classification -> ContentExtractor to output scenes -> (maybe OntologyCurator auto-adds a new type if something novel was found) -> ScheduleAgent schedules scenes -> BudgetAgent updates cost -> ComplianceAgent checks rules. This end-to-end dry run will reveal if any agent isn’t picking up what the previous one left off (meaning interface mismatch or missing trigger). Refine accordingly.

In essence, these changes will create a more loosely coupled but coordinated multi-agent system. Agents will still have single responsibilities, but thanks to well-defined interfaces and orchestrated hand-offs, they will function as an ensemble, each contributing to building or updating different parts of a file’s knowledge cluster. This improves the overall autonomy of the system – after initial triggers, agents can carry the baton one after another with minimal human intervention, all while keeping their scopes clear and using the shared graph as the source of truth for communication.

Cross-Layer Relationship Integration & Silo Reduction

Current State: While the system recognizes distinct layers (IRL, Idea, Ops, Provenance), in practice some of these remain siloed or only loosely connected. For instance, the Idea layer (creative content like scenes, characters) and the IRL layer (production reality like cast, crew, schedule dates) have entities defined but their interconnections are not fully modeled. A scene in the script might correspond to a shoot day on the schedule and involve certain cast/crew, but unless these links are explicitly stored in the graph, the knowledge remains fragmented. In the current schema, we saw hints of this fragmentation: e.g., a PurchaseOrder can list a scene_id and a crew_role text, but those are not actual relationships to Scene or Crew nodes ￼. Similarly, there’s a location_id in ShootDay and a character breakdown file type listed, implying location and character data exist, but it’s unclear if Location nodes or Character->Talent mappings are implemented. The risk is that each layer is populated (we have Scenes, and we have ShootDays, and we have Talents) but they don’t know about each other without manual correlation, which defeats the purpose of a unified knowledge graph.

Needed Changes: The oversight here is the lack of robust links between layers, leading to siloes. To eliminate these silos:
	•	Connect the Dots Across Layers: Ensure every concept that logically crosses layers is represented by actual graph relationships. A quick audit: Characters (Idea) should link to Talent (IRL) who play them; Scenes (Idea) should link to the ShootDay (IRL) on which they’re filmed (and possibly to the PurchaseOrder/expenses (Ops) associated with them); Crew (IRL) should link to Departments or specific tasks (Ops) as needed; Compliance rules (Ops) might link to the Content or IRL elements they apply to (e.g., a compliance rule “Drone permit required for aerial shots” should link to Scenes that are exterior aerials or to a ShootDay if drones are scheduled that day). Where such links are missing, add them. This might also involve creating intermediate nodes for n:m relationships (e.g., a casting node linking Talent and Character if one actor plays multiple characters or vice versa in a project).
	•	Unified Project Ontology View: The four layers should not be treated as four separate graphs, but as one graph with layered substructures. Consider introducing a notion of a Project knowledge graph view (since each project/production is a container of all these layers). In GraphQL, the Project type already aggregates files, content, commits ￼. We can enhance this by ensuring that from a Project node, one can traverse to all related data regardless of layer. For example, Project -> has Scenes -> each Scene has a ShootDay -> each ShootDay has Crew, etc.. If any of these traversal paths are broken due to missing relationships, that’s an oversight to fix. An outcome of this change is that it becomes trivial to answer cross-cutting questions (e.g., “Which scenes are not yet scheduled?” or “Which shoot days involve characters that currently have no cast?”) because the graph has direct links instead of needing out-of-band joins.
	•	Avoid Layer Isolation in Agent Logic: On the process side, make sure that no component is exclusively focusing on one layer without regard for others when it should be. For example, if the BudgetComposer is creating a budget, it shouldn’t do so in a vacuum of Ops numbers; it should pull from Idea (what needs to be budgeted) and maybe IRL (actuals from production) for comparison. Reducing silos means encouraging each process to consider the connected data. This is more of a philosophy, but it can be concretized by writing cross-layer queries for agents and by creating dashboards or views that explicitly show combined data (perhaps a “Project dashboard” that surfaces linked info from all layers).
	•	Consistency Between Layers: Introduce mechanisms to keep layers in sync. One example oversight might be if a scene is deleted or moved in the script (Idea change), the corresponding schedule entries (IRL) and budget entries (Ops) might become outdated. The system should catch such discrepancies. This could be done via integrity constraints (soft ones enforced by agents): e.g., a nightly agent job that verifies every Scene has either a corresponding ShootDay or a flag that it’s not scheduled, and flags any that violate this. Similar checks: every Character in a script should either be linked to a Talent or marked as “uncast”, etc. These checks address the silo issue by ensuring no layer element is “orphaned” from its counterparts in other layers if it should have one.

Actionable Recommendations:
	•	For each CanonicalSlot/file type, map out the ideal cross-layer links its content should have. Use that as a checklist to fill in relationships. For example, a Call Sheet file implies: Scenes <-> ShootDay links, Crew <-> ShootDay links, Talent <-> ShootDay links. Ensure the data model has those and the ingestion process creates them (or that an agent like CallSheetComposer ensures these links when generating or reading a call sheet).
	•	Implement a graph constraint (application-level): no Scene should exist without a link to a Project and (after scheduling phase) a link to a ShootDay. No Character should exist without either a link to a Talent or an open casting issue logged. No ShootDay should exist without linking to at least one Scene (except perhaps wrap-up days). These rules can initially be documented and later enforced by the OntologyCurator (via validation queries) or by triggers. Write Cypher queries to detect violations of these expectations and integrate them into health checks or curator agent routines.
	•	Cross-layer Navigation in UI/Tools: Although not code architecture per se, ensure the user interface or any debugging tools also reflect cross-layer clusters. For example, if there’s an admin UI showing a file’s details, include not just file metadata but also a summary of its linked content (scenes, etc.), ops (related budgets or POs), and provenance (recent commits on it). This will reinforce the cluster concept in practice and quickly expose if something is isolated incorrectly (for instance, if a script file shows zero linked scenes, that’s a red flag the extraction didn’t run).
	•	Provenance for Cross-Layer Changes: When a change in one layer necessitates changes in another, use the provenance log to tie them together. For instance, if moving a Scene from one ShootDay to another, perform both the scene-schedule link update and any budget adjustments in one commit or in a series of commits with references to each other. This way the provenance layer can be queried to understand cross-layer operations (e.g., “this commit moved Scene 5 from Day 3 to Day 4 and updated related catering costs”). Ensuring the provenance captures multi-layer transactions will help audit that silos are truly being broken (we can see a single coherent event spanning layers, rather than separate unrelated commit histories in each layer).

In summary, breaking the silos requires both schema links and process vigilance. Many of the pieces are already in place in V11’s design (the concept of layers is there, and some linking mechanisms like scene_id in POs or content references exist). The task now is to solidify those into first-class graph connections and to adjust workflows so that the layers behave as one system. After these changes, the IRL, Idea, and Ops layers will function as an integrated ontology web, with files serving as convenient entry points and provenance providing oversight on all inter-layer interactions.

Summary – Alignments and Key Refactor Priorities

In undertaking this audit, we identified areas needing fundamental rework, refinement, and those already on the right track:
	•	Fundamental Rework: The File ingestion and content extraction flow is the most significant overhaul – it must transition from one-node file entries to generating entire subgraphs of knowledge per file. This underpins everything else. Additionally, the agent orchestration model needs a fundamental shift to support multi-agent workflows and cluster-aware processing rather than isolated task handling.
	•	Refinements & Clarifications: The ontology schema largely supports the envisioned model (e.g., use of EdgeFacts, presence of multi-layer entity types, etc.), but it needs refinement in the form of added relationships, clearer type layering, and possibly simplifying the content model (decide on generic vs specific content node usage). Likewise, agent roles should be clarified (e.g., define new agent responsibilities for content parsing or cross-link maintenance) to ensure no aspect of cluster upkeep is unassigned. These are not complete redesigns but rather targeted improvements on the existing foundation.
	•	Areas of Good Alignment: The use of a Neo4j graph with explicit ontology layers and temporal provenance is a strong architectural choice that aligns well with the “cluster” concept – it’s inherently a network of nodes. Features like EdgeFact temporal relationships for file classification ￼ and the commit/versioning system for all changes mean the platform can already record nuanced, multi-node relationships over time (which is exactly what clusters need for evolution). The presence of an OntologyCuratorAgent is also forward-thinking ￼; it will be instrumental in governing the schema as new node types or links are required for cross-layer integration (ensuring the ontology can evolve safely as cluster complexity grows). The modular agent design (different agents for different concerns) is a good match for cluster management, provided we implement the glue for their collaboration as noted.

By implementing the recommendations above – expanding the file ingestion into a cluster builder, empowering agents to traverse and update all relevant layers, extending the schema for full cross-layer linkage, and enabling richer agent coordination – Olivine’s system will embody the new conceptual direction. Each file will truly become an entry point to a dynamic cluster of knowledge, and the agents will operate as maintainers and navigators of these clusters, rather than as siloed automation scripts. This will improve consistency across the IRL, Idea, Ops, and Provenance layers and unlock far more insight and automation potential from the interconnected data ￼. The result will be a robust, ontology-driven backbone where a single change (in a file or in the world) propagates appropriately through all representations, with the provenance to audit every step – exactly the outcome envisioned by the shift to “files as node clusters.”