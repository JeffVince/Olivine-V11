## **1\. Canonical Knowledge Model (Ontologies)**

At the heart of the system is a **canonical knowledge graph model** that represents four layers of truth (File, Content, Ops, and Provenance). These ontologies are implemented in Neo4j (a graph database), meaning all entities are nodes with typed relationships. By unifying data in a graph, we can naturally model many-to-many relationships (e.g. a Scene involves multiple Props and Characters, a Crew member might fulfill multiple Roles) and traverse connections easily. The graph is **single-source-of-truth** for the production's state.

### **Graph DDL: Constraints & Indexes**

We standardize identities and hot paths with explicit Neo4j DDL (Neo4j 4.4/5 syntax). These constraints/indexes back the versioning and edge‑history mechanics described below.

```cypher
// === Uniqueness constraints (nodes we identify by stable keys) ===
CREATE CONSTRAINT unique_commit        IF NOT EXISTS FOR (c:Commit)          REQUIRE c.id IS UNIQUE;
CREATE CONSTRAINT unique_action        IF NOT EXISTS FOR (a:Action)          REQUIRE a.id IS UNIQUE;
CREATE CONSTRAINT unique_org           IF NOT EXISTS FOR (o:Org)             REQUIRE o.id IS UNIQUE;
CREATE CONSTRAINT unique_project       IF NOT EXISTS FOR (p:Project)         REQUIRE p.id IS UNIQUE;
CREATE CONSTRAINT unique_source        IF NOT EXISTS FOR (s:Source)          REQUIRE s.id IS UNIQUE;
CREATE CONSTRAINT unique_folder        IF NOT EXISTS FOR (f:Folder)          REQUIRE f.id IS UNIQUE;
CREATE CONSTRAINT unique_file          IF NOT EXISTS FOR (f:File)            REQUIRE f.id IS UNIQUE;
CREATE CONSTRAINT unique_slot          IF NOT EXISTS FOR (s:CanonicalSlot)   REQUIRE s.key IS UNIQUE;
CREATE CONSTRAINT unique_profile       IF NOT EXISTS FOR (p:TaxonomyProfile) REQUIRE p.id IS UNIQUE;
CREATE CONSTRAINT unique_rule          IF NOT EXISTS FOR (r:TaxonomyRule)    REQUIRE r.id IS UNIQUE;
CREATE CONSTRAINT unique_edgefact      IF NOT EXISTS FOR (e:EdgeFact)        REQUIRE e.id IS UNIQUE;
CREATE CONSTRAINT unique_branch        IF NOT EXISTS FOR (b:Branch)          REQUIRE b.name IS UNIQUE;

// === Common B-tree indexes for hot lookups / temporal queries ===
CREATE INDEX file_org_id          IF NOT EXISTS FOR (f:File)          ON (f.org_id);
CREATE INDEX project_org_id       IF NOT EXISTS FOR (p:Project)       ON (p.org_id);
CREATE INDEX edgefact_lookup      IF NOT EXISTS FOR (e:EdgeFact)      ON (e.type, e.from_id, e.to_id, e.valid_to);
CREATE INDEX entityversion_lookup IF NOT EXISTS FOR (v:EntityVersion) ON (v.entity_id, v.entity_type, v.valid_to);
```

> Note: if you're pinned to Neo4j < 4.4, convert to legacy syntax (`CREATE CONSTRAINT ON (n:Label) ASSERT n.id IS UNIQUE` and `CREATE INDEX FOR (n:Label) ON (n.prop)`). Keep the composite `EdgeFact`/`EntityVersion` indexes intact—they power "current row" lookups (`valid_to IS NULL`) efficiently.

### 1.1 File Ontology (Storage Truth)

 This layer models files/folders in external storage (Dropbox, Google Drive, or the app’s own storage) and maps them into a unified graph structure. Every file and directory is represented as a node, annotated with its storage metadata, and linked to canonical categories via taxonomy. The implementation uses Neo4j with unique IDs to avoid duplicates, and a sync process keeps it consistent with the source of truth (Supabase/Postgres and the connected storage APIs).

* **Node Types & Properties:**

  * **:`Source`** – A storage provider (e.g. Dropbox, Google Drive, Supabase Storage). Properties include `id` (UUID), `org_id` (UUID for multi-tenant scoping), `name` (user-friendly label), `provider` (e.g. "dropbox", "gdrive"), `provider_account_id` (the account ID from the provider), and `token_ref` (opaque reference to external secrets). **SECURITY:** Provider tokens and refresh secrets are stored **only** in Vault/KMS, never in the graph. The graph keeps `Source.token_ref` (opaque), `provider_account_id`, and `org_id`. Rotation flows update Vault/KMS; the graph ref is stable. The Source node represents a connection to an external storage system. For example, a user's Dropbox account would be one Source node with provider="dropbox" and the account ID. The sync service resolves `token_ref` via Vault/KMS right before provider API calls. A unique constraint on `(Source.provider_account_id, Source.org_id)` ensures no duplicate provider accounts within an organization.

  * **:`Folder`** – A folder/directory in the storage hierarchy. Properties: `id` (UUID primary key), `org_id` (UUID for multi-tenant scoping), `db_id` (integer from Supabase for relational lookups), `name` (folder name), `path` (full path or logical path within the source), and `parent_id` (UUID of parent Folder if nested, or `NULL` if it's a root folder). A unique constraint on `(Folder.db_id, Folder.org_id)` is enforced in Neo4j to prevent duplicates within tenant boundaries. Folders mirror the structure of the user's storage. For example, a Dropbox “Production123” folder and its subfolders would each appear as a Folder node scoped to the organization.

  * **:`File`** – An actual file (document, image, etc.). Properties include `id` (graph-native ULID/UUIDv7), `org_id` (UUID for multi-tenant scoping), `db_id` (integer from Supabase for relational lookups), `name` (filename), `path` (full path within its source), `size` (bytes), `mime_type` (e.g. `"application/pdf"`), and `bucket_id` (identifier of the storage bucket or source container). Timestamps `created_at` and `updated_at` are also stored. The sync service populates a `metadata` JSON field with any extra attributes (e.g. checksums or tags), but **excludes any provider tokens or credentials** which are managed externally. **CRITICAL:** `File.db_id` corresponds to the Supabase UUID; `File.id` is a graph-native ULID/UUIDv7; ingestion MERGEs on `db_id` with org_id stamping for tenant isolation. A Neo4j constraint ensures `(File.db_id, File.org_id)` is unique within tenant boundaries, so if the same file record is moved or renamed (retaining the same db_id), the graph node is updated rather than duplicated. Uniqueness of `(org_id, source_id, path)` is enforced in SQL; graph deduplicates by `db_id`.

  * **:`CanonicalSlot`** – *(Planned)* An abstract node representing the standardized “slot” or category a file can fill (e.g. **SCRIPT\_PRIMARY**, **BUDGET\_FINAL**, **CALL\_SHEET\_DRAFT**). Properties include `key` (unique slot identifier), `org_id` (UUID for multi-tenant scoping), and `description` (human-readable explanation). This is not yet implemented in code (no `CanonicalSlot` nodes exist in the current schema – **GAP**), but conceptually each CanonicalSlot would be scoped to an organization with a unique constraint on `(CanonicalSlot.key, CanonicalSlot.org_id)` to allow different organizations to define their own slot taxonomies. CanonicalSlots serve as targets for classification: e.g. the file **`shooting_script_v3.pdf`** might be tagged as filling the **SCRIPT\_PRIMARY** slot of the project within that organization's taxonomy.

  * **:`TaxonomyRule`** – *(Planned)* A rule node encoding a pattern or criteria to auto-classify files. Properties include `id` (UUID), `org_id` (UUID for multi-tenant scoping), `match_pattern` (regex or glob for filenames/paths), `slot_key` (which CanonicalSlot to assign), `file_type` (optional MIME type filter), `priority` (integer for rule ordering), and `enabled` (boolean flag). These rules would be data-driven configurations (instead of hard-coded logic), enabling customization per project or org with tenant isolation. A unique constraint on `(TaxonomyRule.id, TaxonomyRule.org_id)` ensures proper scoping. (No such nodes or config exist yet – **GAP**).

  * **:`TaxonomyProfile`** – *(Planned)* A profile node that groups a set of TaxonomyRules for a given organization, project, or region. Properties include `id` (UUID), `org_id` (UUID for multi-tenant scoping), `name` (profile identifier), `description`, and `active` (boolean). For instance, a studio might define a profile with rules: *"any file in folder 'Schedules' OR with name matching `*callsheet*` → CALL\_SHEET\_DRAFT slot."* Profiles would allow different productions to have different naming conventions within organizational boundaries. A unique constraint on `(TaxonomyProfile.name, TaxonomyProfile.org_id)` ensures tenant-scoped uniqueness. (Not implemented – **GAP**).

* **Relationships:**

  * **(:Source)-\[:ROOT\]-\>(:Folder)** – Each Source has a root Folder. In the graph, we plan to link a Source node to the top-level Folder node that represents the entire contents of that source (**GAP**: The current implementation doesn’t explicitly create this link, but it is implied by `Folder.parent_id = NULL` for root. We should add an explicit ROOT relationship for clarity). For example, if a Dropbox account (Source) contains a folder “ProjectX”, the Source node would have a `ROOT` edge to the “ProjectX” Folder node.

  * **(:Folder)-\[:CONTAINS\]-\>(:File)** and **(:Folder)-\[:CONTAINS\]-\>(:Folder)** – Represents the hierarchy: a Folder contains files and subfolders. This is implemented by the sync worker when processing file events. On file creation or update, the worker ensures a relationship from the parent Folder to that File in Neo4j. For example, a Folder node “Scenes” \-\[:CONTAINS\]-\> a File node “scene\_list.pdf”. Similarly, when a subfolder is created, the parent folder will get a CONTAINS link to it. These edges make folder traversal possible in the graph. (Neo4j essentially stores a tree of folders/files mirroring the storage.) The relationship may carry a property (like `relationship_type: "contains"` as in code), but that is mainly descriptive. The key is that the graph structure reflects the real folder structure.

  * **(:File)-\[:BELONGS\_TO\_FOLDER\]-\>(:Folder)** – Inverse of the above, capturing each File’s parent folder. The schema design includes this for quick backward lookup. In practice, one can query the inverse of CONTAINS, but an explicit relationship could be stored for convenience. **Note:** The current sync implementation does **not** separately create a BELONGS\_TO\_FOLDER edge because Neo4j relationships are inherently directed; a CONTAINS from Folder to File can be traversed in reverse. We may treat BELONGS\_TO\_FOLDER as implicit. (If needed for clarity, an APOC trigger or additional upsert could add it – currently **GAP**, not strictly required since `CONTAINS` is enough to navigate upward).

  * **(:File)-\[:CANONICAL\_SLOT\]-\>(:CanonicalSlot)** – Links a file to the abstract “slot” it fulfills. **GAP:** This classification is not automatic yet. The design calls for, for example, a file whose name matches a script pattern to get a relationship to the **SCRIPT\_PRIMARY** slot node. In the current system, such tagging would have to be done manually or via future automation. As a future fix, the TaxonomyRules could be applied whenever a File node is created/updated: e.g. a rule matches `shooting_script` in the name and assigns the CanonicalSlot “SCRIPT\_PRIMARY” by creating a `CANONICAL_SLOT` edge. This way, the graph can answer queries like “find the primary script file for the project” by traversing to the slot node.

  * **(:TaxonomyProfile)-\[:USES\_RULE\]-\>(:TaxonomyRule)** – Associates profiles with their constituent rules. If an organization has a taxonomy profile, that Profile node would have multiple USES\_RULE edges to various TaxonomyRule nodes (each rule encapsulating one naming convention). This allows different productions to plug in different rule sets. (Not yet in implementation – **GAP**). In the future, we might also link a Project to a TaxonomyProfile (e.g., `(:Project)-[:USES_TAXONOMY]->(:TaxonomyProfile)`) so that the system knows which set of rules to apply for that project’s files (**FIX:** add a field in project or organization settings to select the taxonomy profile, then use it during file ingestion to classify files accordingly).

**Implementation & Rationale:** All File/Folder/Source entities are ingested from the Supabase DB via the **sync worker** with **multi-tenant isolation**. Whenever a file or folder record is inserted/updated in Postgres, a `SyncEvent` is emitted. The `FileHandler` and `FolderHandler` in the sync worker translate those events into Neo4j upsert operations using **MERGE on db_id with org_id stamping**. For example, on a new file, the FileHandler calls:

```cypher
MERGE (f:File {db_id: $file_db_id, org_id: $org_id})
  ON CREATE SET f.id = $new_ulid, f.created_at = $now
  SET f.name = $name, f.path = $path, f.size = $size, f.updated_at = $now
```

This approach ensures tenant isolation while using the Supabase `db_id` as the primary merge key and maintaining a graph-native `id` (ULID/UUIDv7). The sync worker then calls `upsert_relationship` to maintain folder→file links within the same tenant boundary. **SECURITY:** Provider tokens and credentials are never stored in the graph but are managed externally via Vault/KMS with only opaque `token_ref` stored in Source nodes. When handling `Source` updates, write `token_ref` only; resolve real tokens via Vault/KMS right before provider API calls. The unique constraint on `(File.db_id, File.org_id)` prevents duplicate nodes within tenant boundaries – if a file is moved or renamed (retaining the same db_id), the graph node is updated rather than duplicated. Similarly, deleting a file triggers tenant-scoped deletion that removes the node and any attached relationships within that organization's boundary.

**API Layer Enforcement:** Add a **mandatory `org_id`** to every API request context (JWT claim or header → validated). All Cypher templates must include org scoping, e.g.:

```cypher
MATCH (p:Project { id: $project_id, org_id: $org_id })
MATCH (p)-[:HAS_FILE]->(f:File { org_id: $org_id })
...
```

**Ban direct driver access** from clients. Only the API talks to Neo4j; the API injects `org_id` and validates project/org ownership on every route.

The File Ontology abstractions make it possible for the application to treat storage in a uniform way. Regardless of whether a file lives in Google Drive or on local storage, queries like “find all Files for Project X that are in the ‘Concept Art’ slot” become feasible: the graph can traverse from a Project node to all its File nodes (via project relationships), then filter those that have a `CANONICAL_SLOT` of “ConceptArt”. Likewise, because every folder and file is a node, the system can easily answer questions like “how many files are in this folder (including subfolders)” or detect duplicates across sources by matching properties (or in future, checksums). The ontology’s power is in linking unstructured storage to structured categories: e.g. once a **Call Sheet** PDF file is tagged with slot **CALL\_SHEET\_DRAFT**, the Content and Ops layers (below) can reference it in their workflows (for instance, the Ops layer might ensure an **InsuranceDoc** file exists whenever a **StuntScene** appears – by checking for a File in the **INSURANCE\_CERTIFICATE** slot).

**GAP:** The Taxonomy-based auto-tagging is not yet implemented. Currently, classification of files (e.g. deciding which file is the “primary script”) may be done manually by the user (or not at all). **FIX:** To fully realize the File ontology, we should implement taxonomy rules processing. A potential implementation: maintain a set of TaxonomyRule nodes (or a config JSON) where each rule has a regex (e.g. `(?i)^(shooting_)?script.*\\.pdf$`) or a folder path condition, plus a target CanonicalSlot. After a file sync upsert, run these rules against the file’s name/path; on a match, create the `(:File)-[:CANONICAL_SLOT]->(:CanonicalSlot)` relationship. Store the rules in the DB (perhaps a `taxonomy_rules` table and `taxonomy_profiles` table linking to projects or orgs) so they can be managed without code changes. This way, the system can adapt to different projects’ naming conventions and automatically populate the graph with meaningful tags that the rest of the pipeline can use.

### 1.2 Content Ontology (Creative Truth)

This layer captures the creative elements of the production – essentially the world and plan described by the script and related documents – in a structured graph form. It connects the narrative content (scenes, characters, etc.) with production planning (schedules, crew assignments) and also links back to the File layer where needed. The content ontology enables queries about the story and production: e.g. “Which characters appear in Scene 5 and what props do they use?” or “What scenes are scheduled for Shoot Day 3, and which locations are involved?”

* **Node Types:**

  * **:`Project`** – The top-level entity representing a film/episode/production. In the DB schema it corresponds to a row in `projects` and is synced to a Neo4j node with label Project. Properties include `id` (UUID), `name`, `description`, and `user_id` (the owner/creator), plus timestamps. In the graph, a Project is the container for all content of that production.

  * **:`Script`** – A screenplay or script for the project. This node represents the narrative content of the production. (In implementation, we currently do **not** have a distinct `scripts` table – the primary script might simply be a File in the File ontology marked as the “Script”. **GAP**: a formal Script node is not created yet on sync. We may treat the script as a concept extracted from the file.) In a full ontology, a Script node would have properties like `id` (could reuse the File ID or a separate UUID), `title`, and possibly `draft_number` or version. A Project could have one or multiple Script nodes (for instance, original script and revisions).

  * **:`Scene`** – A scene in the script, representing a contiguous part of the story (often one location and time). Scene nodes would have properties like `scene_number` (or identifier), `header` (e.g. *INT. POLICE STATION – DAY*), `location_name` (name of the setting, if any), `time_of_day` (DAY/NIGHT), and possibly page count or duration. This is not yet stored persistently, but we **do** generate scene data during script breakdown. In the breakdown pipeline, after parsing a script, each scene is represented in memory (and even synced to Neo4j in a “breakdown” context). For example, the `scene_sync_service_neo4j` simulates creating `BreakdownScene` nodes with properties scene\_number, header, etc. These correspond to what permanent Scene nodes would be. Currently, those breakdown scene nodes are prefixed (e.g. `BreakdownScene`) and tied to a specific analysis session, not globally unique or reused. The plan is to have actual `:Scene` nodes in the canonical graph once a script is ingested/approved. Each Scene node would likely be uniquely identified by a combination of Project and scene number (or an auto-generated ID) so it can be versioned if the script changes.

  * **:`Beat`** – *(Optional detail)* A story beat within a scene. The ontology includes Beats to represent finer-grained narrative moments (like comedic beats, dramatic turns). This is not implemented in code – it’s an extension point for future where scene text might be further broken down. If used, a Beat node might have properties like `summary` or `type` (e.g. “comic relief beat”), and would link to its parent Scene. (No current support – **GAP**).

  * **:`Prop`**, **:`Location`**, **:`Character`** – Key creative elements extracted from the script: props (objects used), locations (places/settings), and characters (roles in the story). Each of these would be a node in the graph, allowing many-to-many relationships with scenes. For example, “Police Station” (Location node) could be linked to all Scene nodes set in a police station; “Revolver” (Prop node) might link to Scenes where a gun is used; “Detective John” (Character node) links to all scenes the character appears in. In the breakdown process, the system identifies these elements: characters are found by parsing dialogue headings, props/locations might be identified via heuristics or manual tagging. In the current breakdown code, all these are captured as `ProductionElement` or categorized lists rather than first-class graph nodes. Specifically, the breakdown state has `production_elements` with types like “prop”, “location”. During syncing, it creates generic `ProductionElement` nodes for each (with an `element_type` field to distinguish). **Current Implementation Note:** All props, locations, costumes etc. are coming in as `ProductionElement` nodes in the temporary breakdown graph with a type property, not as separate labels. The ontology calls for distinct labels (:Prop, :Location, etc.), which would be more semantically rich. A future implementation should instantiate the specific node labels for clarity (or at least set a label per type in addition to the generic one). For now, we treat Prop/Location as conceptual categories – in practice they are differentiated by `element_type` in the breakdown output. Each such node would typically have at least a `name` or `description`.

  * **:`Talent`** (or **:`CrewMember`**) – A real person (actor) who portrays a Character, or a crew person filling a role. The ontology distinguishes the fictional character from the real person playing them. A Talent node would represent an actor (with properties like name, maybe agent info or union affiliation). Similarly, CrewMember could represent behind-the-camera personnel. In the current system, we have a `Crew` entity in the DB which is somewhat generic (it might represent a group or an individual crew). Each Crew has a `name` and `user_id` (owner). There isn’t a separate “Talent” table; likely, actors could be represented as Crew records or simply as entries in a cast list that become Crew nodes. For now, we’ll assume **Crew** nodes double as people (be it cast or crew). The **CrewRole** vs **CrewMember** distinction in the ontology is a design nuance: one could model a “Role” (like *Director* or *Lead Actor*) as an entity, and then link a person to that role. However, the current implementation doesn’t have a CrewRole node type – instead, the role is an attribute on the relationship between a Crew (person) and a Project. In the graph schema mapping, the `project_crew` junction yields an `ASSIGNED_TO` relationship from Crew to Project with a `role` property. That means if *Alice* is the Director on Project X, we create (Crew:`Alice`)-\[ASSIGNED\_TO `{role: "Director"}`\]-\>(Project X). This suffices to mark her role without a separate CrewRole node. (If needed, CrewRole nodes like “Director” could be added to standardize roles across projects, but currently roles are free-text properties).

  * **:`ShootDay`** – A scheduled shooting day. If the production schedule is planned out (which scenes to shoot on each day), each day can be a node with a date and perhaps an index (Day 1, Day 2 of filming). ShootDay nodes would be connected to the scenes slated for that day (see relationships below). This ontology element isn’t in the database yet – likely the schedule would come from an imported shooting schedule or manual input. Currently, no ShootDay nodes are created (so **GAP**). We anticipate adding a scheduling module which, once a production schedule is made, creates ShootDay nodes with properties like `date`, `index` (shoot day number), and maybe `call_time`, etc.

  * **:`Schedule`** – Represents an overall production schedule or a document like a stripboard. This could be an entity that groups ShootDay nodes or holds meta info (e.g. version of schedule, start and end dates). It might not be strictly necessary as a separate node if all info is on ShootDay and Project, but the ontology lists it possibly to hold things like schedule versioning. In practice, a “Schedule” might correspond to a file (like *Schedule\_v2.pdf*); the ontology would relate that file to a Schedule node (see Template below). This is conceptual for now (no explicit node in code).

  * **:`Template`** – A document template, such as a call sheet template, budget template, etc. This is a kind of content resource that is not part of the narrative but is a reusable format. Template nodes could be used to generate new documents. The system might have predefined Templates (e.g., a call sheet layout). In the graph, a Template node would likely be linked to an actual File that stores the template file (for example, a Google Sheets template for budgeting). The ontology suggests `(:Template)-[:STORED_AS]->(:File)` for that link. At present, templates are not fully modeled (they might just exist as files in a “Templates” folder), so we haven’t seen Template nodes in code – **GAP** until we formalize it.

  * **(:ShootDay)-\[:COVERS\_SCENE\]-\>(:Scene)** – Indicates that a particular shoot day is scheduled to cover (film) a given scene. In shooting schedules, scenes are often assigned to specific dates. By encoding this, we can ask "on which day will Scene 5 be shot?" or "which scenes are being shot on Day 3?". This requires a scheduling step. Currently, since scheduling isn't integrated, no such edges exist. Once implemented, creating these relationships would likely happen when a schedule is imported or created. For example, if ShootDay node (date 2025-08-09) covers scenes 4,5,6, we'd have three COVERS\_SCENE edges from that ShootDay to Scene4, Scene5, Scene6. **This is maintained as a materialized current edge; the authoritative history is `:EdgeFact {type:'COVERS_SCENE'}` with `valid_*` and `branch`. The scheduler and imports must write EdgeFacts, not direct rels.**

  * **(:CrewRole)-\[:ASSIGNED\_TO\]-\>(:Talent or :CrewMember)** – Links a role to the person fulfilling it. If we explicitly model CrewRole nodes (like "Director", "Gaffer", "Lead Actor"), this relationship assigns a CrewMember or Talent to that role. However, as noted, the implementation currently encodes role assignment differently: it uses an `ASSIGNED_TO` (or `BELONGS_TO_PROJECT`) relationship from the person to the project with a role property. For example, Crew Alice \-\[:ASSIGNED\_TO {role:"Director"}\]-\> Project X, rather than separate node for "Director". The ontology description allows for a CrewRole node if we needed to treat the role itself as an entity (say, to attach rate cards or requirements to the role). In absence of that, the system still captures who is in what role via relationship properties. For documentation's sake: if we did have CrewRole nodes, the link might look like (CrewRole "Director") \-\[:ASSIGNED\_TO\]-\> (CrewMember Alice), and possibly also (Project)-\[:HAS\_ROLE\]-\>(CrewRole "Director") to tie the role to the project context. This is a more complex model. The simpler model (currently used) is one hop: Project \<-\[:ASSIGNED\_TO {role:"Director"}\]- Crew. We will stick to the implemented approach unless a need arises to query roles independently. **When an assignment is time‑varying (e.g., person fills role only for a window), write `:EdgeFact {type:'ASSIGNED_TO'}` between Crew (from) and Project (to) with `props.role`, and maintain `(:Crew)-[:ASSIGNED_TO {role}]->(:Project)` as current. For static "belongs" you may keep a plain rel.**

  * **Links to Files:** The content ontology doesn’t live in isolation – it connects to the File layer for any documents that represent or reference creative elements. Two notable relationships:

{{ ... }}

    * **(:Template)-\[:STORED\_AS\]-\>(:File)** – Associates a Template node with the actual file that contains the template content. For instance, a call sheet template (Template node) might be stored as a Google Sheet or PDF (File node). If the user has a file that is a template, we would create a Template node for it and connect them. Similarly, the “final shooting schedule” might be a PDF file – we could represent that by a Template or Schedule node linked to the File. In practice, we haven’t seen template nodes; an uploaded template is just a File. Marking it in graph would require either user input or a convention (perhaps files in a “Templates” folder could automatically get a Template node). This is a minor part of the ontology, to ensure even these meta-documents are linked in.

* **Explanation & Usage:** The content ontology bridges the gap between **documents** and **data**. By modeling scenes, characters, etc., as first-class nodes, the system can answer complex questions that join creative and logistical information. For example, consider the query: *“What scenes are scheduled on 2025-08-09, and which props and locations do they involve?”* Using the graph, this would work by finding the ShootDay node for Aug 9 2025, getting its COVERS\_SCENE relationships to Scene nodes, then from each Scene following NEEDS\_PROP to Prop and SET\_AT to Location. If those Props or Locations have any files (images, reference docs), we could further traverse REFERENCED\_BY to get those File nodes (and thus URLs to view them). This is extremely powerful compared to searching through spreadsheets or script PDFs manually.

In the current implementation, some pieces of this ontology are already realized:

* Projects and Crew (people) are fully implemented and synced to the graph. We can query which crew are associated with a project and even see their role from the relationship property. For example, a Cypher query `MATCH (p:Project {id:$id})<-[r:ASSIGNED_TO]-(crew:Crew) RETURN crew.name, r.role` would list team members and their roles.

* Scenes, Characters, Props, etc., are *partially* realized through the script breakdown pipeline. When a user runs a script breakdown, the system parses the script into scenes and elements and (in the latest version) writes a temporary subgraph. It creates nodes for scenes and characters (with a prefix “Breakdown” label) and links them with relationships like APPEARS\_IN\_SCENE and USED\_IN\_SCENE. It even groups them under a breakdown “ledger” node via CONTAINS\_SCENE. However, these nodes currently exist only in the context of that analysis session (often on a separate in-memory graph or a Neo4j transaction that might not be merged into the main graph). They are not yet persisted as global content knowledge. This means after the breakdown, the insights (like which props are in which scene) aren’t permanently queryable unless we explicitly merge them. **FIX:** Going forward, we want to merge the breakdown results into the canonical graph (possibly guarded by a user confirmation). That would involve converting `BreakdownScene` nodes into official `Scene` nodes (or updating/creating Scene nodes for the Project), `BreakdownCharacter` into `Character` nodes, and so on, avoiding duplicates (e.g., if a Character node already exists for “Detective John” in that project, link to that instead of new). We also need to decide how to version these (if the script changes, see Provenance section below). In summary, the content ontology is designed, and the data is available to populate it; implementing the ingestion of that data into the permanent graph is the next step.

Finally, by linking the content layer to files and the ops layer, we maintain **traceability**. For instance, if a Scene node has a NEEDS\_PROP “Revolver”, and there’s a File node representing a prop list spreadsheet, we could link that File to the Prop node or Scene node to say “this spreadsheet lists the props for scene X”. Similarly, if a call sheet document (File) mentions which scenes are being shot, we could link that call sheet’s File node to the corresponding ShootDay or Scene nodes. All these cross-links enrich the graph so that an AI agent or user can traverse from a high-level concept to the exact document that contains details, and vice versa. The Content ontology thus creates the **semantic backbone** of the production – the who, what, where of the story – which all other parts of the system refer to as the source of truth for creative details.

### 1.3 Ops Ontology (Operational Truth)

Where the content ontology deals with the creative side, the Ops ontology covers the **logistical, financial, and compliance** aspects of the production. This includes things like purchasing, budgeting, time tracking, and regulatory compliance – all of which are critical to real-world production management. By representing these as graph nodes and relationships, we can connect them back to the creative elements (for context) and to each other for end-to-end visibility (e.g., linking a purchase order to the scene it was for, or linking a timesheet to the shoot day and crew involved).

* **Node Types:**

  * **:`Vendor`** – An external vendor or company, such as an equipment rental house, caterer, or any service provider. Properties might include `id` (UUID or name), `name`, `contact_info`, `category` (e.g. EquipmentRental, Catering, etc.). (In the current system, no Vendor table exists – **GAP**. We anticipate adding one when financial integration is tackled.)

  * **:`PO`** (Purchase Order) – Represents an order or expenditure authorization, e.g., renting equipment or buying props. Key properties: `id` (could be a PO number or UUID), `description` or `item`, `amount`, `date`, and possibly `vendor_id` (link to Vendor), and references to what it’s for (scene or department). In practice, POs might be stored in a finance system; if we integrate, we’d have a table or API feed for POs. (Not implemented yet – **GAP**).

  * **:`Invoice`** – A bill from a vendor. Properties: `id` (invoice number), `amount`, `date`, `due_date`, `status` (e.g., pending, paid), `vendor_id`. In a graph context, Invoice nodes link to the corresponding POs or vendor. (Not yet in system – **GAP**).

  * **:`Timesheet`** – A timesheet entry, typically for cast/crew work hours. Properties: `id`, `crew_id` (who the timesheet is for), `date` or date range, `hours`, `overtime_hours`, etc. Could also include `role` or position of that person for context, and a reference to which shoot day or scene it covers if applicable. (No timesheet feature in current system – **GAP**).

  * **:`PayrollBatch`** – A payroll run that includes multiple timesheets, used to process payments for a period. Properties: `id`, `start_date`, `end_date`, `total_amount`, `status`. (Not implemented – **GAP**). This would group Timesheet nodes.

  * **:`ComplianceRule`** – A rule or requirement that the production must comply with, such as union regulations, safety rules, or legal requirements. For example, *“Stunts require a certified medic on set”* or *“Child actors cannot work past 10 PM”*. Properties: `id` or name (like “SAG-AFTRA Rule 12.4”), `description`, and perhaps `scope` (whether it’s a union rule, local law, internal policy, etc.). We expect a set of predefined ComplianceRule nodes (possibly loaded from templates or defined per project). (Not in current DB – **GAP**).

  * **:`UnionRateCard`** – Represents pay rates and conditions for union crew/talent positions. E.g., a SAG actor rate card or DGA director rates, which define standard pay, overtime, etc. Properties: could include `union_name`, `role` (position name), `base_rate`, `overtime_rules`, etc. This is a specialized data entity that might be referenced when generating budgets or checking compliance (for minimum pay). (Not implemented – likely would be static data – **GAP**).

  * **:`InsuranceDoc`** – An insurance document or certificate of insurance (COI). Productions require various insurance policies (general liability, workers’ comp, etc.), and specific high-risk activities (like stunts) require that appropriate insurance is in place. We model an InsuranceDoc as a node so we can link it to what it covers. Properties: `id` (could be a policy or certificate number), `type` (e.g. GeneralLiability, StuntInsurance), `valid_from`, `valid_to`. In reality an insurance doc is also a file; likely we’d have an InsuranceDoc node linked to the actual File (similar to Template→File). Currently, insurance docs would just be files in a folder; no explicit node type exists – **GAP** to add one for better tracking.

* **Relationships:**

  * **(:PO)-\[:FOR\_SCENE\]-\>(:Scene)** – Ties a purchase order to a specific Scene if the expense is scene-specific. For example, if you rent a specific prop just for Scene 5, the PO for that rental can be linked to Scene 5\. This contextualizes expenses in terms of the story: one could query “what is the total cost of Scene 5?” by summing amounts of all POs `FOR_SCENE->Scene5`. Implementing this would require the PO record in the database to carry a reference to the scene (perhaps a scene number or ID). Right now, we lack both PO data and persistent Scene nodes, but the ontology supports this link when those exist.

  * **(:PO)-\[:FOR\_ROLE\]-\>(:CrewRole)** – Links a PO to a department or role rather than a scene. For example, “Stunt Coordinator fee” might be a PO linked to the *Stunt Coordinator* role, or “Catering for entire crew” PO might be linked to a general *Catering* category (which could be represented as a CrewRole or just noted in description). In graph terms, if we had a node for the role/department, we attach the PO there. In the current data model, since CrewRole isn’t explicitly separate, another way to implement this is to link PO to a Crew (Crew member) or leave it as a property. However, the ontology’s intent is to capture what part of the production the expense is for (scene or department). **FIX:** When implementing POs, include fields like `scene_id` or `crew_role` so that the sync can create either FOR\_SCENE or FOR\_ROLE edges accordingly.

  * **(:Invoice)-\[:FROM\_VENDOR\]-\>(:Vendor)** – Connects an invoice to the vendor who issued it. This is straightforward: each Invoice node should link to the Vendor node (e.g., Invoice \#1001 \-\[:FROM\_VENDOR\]-\> Vendor “ABC Camera Rentals”). One vendor can have many invoices.

  * **(:Invoice)-\[:PERTAINS\_TO\]-\>(:PO)** – If an invoice is associated with a specific purchase order (or multiple), link them. Often an invoice will reference a PO number. In graph, this creates a chain: Vendor \-\> Invoice \-\> PO \-\> (Scene or Role). This allows tracing costs from the financial document back to the creative purpose. For instance, given an invoice, we could follow PERTAINS\_TO to find the PO, then FOR\_SCENE to see which scene’s expense it was.

  * **(:Timesheet)-\[:FOR\_CREW\]-\>(:CrewMember)** – Associates a timesheet with the crew person (or cast) who logged those hours. E.g., Timesheet T123 \-\[:FOR\_CREW\]-\> Crew node for “Jane Doe”. This makes it easy to get all timesheets for a given person or to link a timesheet entry into other contexts (project, role, etc., through that Crew node). Since Crew nodes are already tied to Project via ASSIGNED\_TO, one can find “all timesheets for Project X” by `MATCH (p:Project {id:X})<-[:ASSIGNED_TO]-(crew:Crew)<-[:FOR_CREW]-(t:Timesheet)`.

  * **(:Timesheet)-\[:COVERS\_DAY\]-\>(:ShootDay)** – Optionally link a timesheet to the shoot day(s) it covers. For daily timesheets, this would point to a ShootDay node (e.g., timesheet for Aug 9, 2025 \-\> ShootDay Aug 9, 2025). If a timesheet spans multiple days (weekly timesheet), it might not use this, or we could have multiple links or a broader interval. The idea is to connect labor hours to the schedule, enabling queries like “how many crew hours were spent on Day 3 of shooting” by aggregating timesheets connected to that ShootDay.

  * **(:PayrollBatch)-\[:INCLUDES\]-\>(:Timesheet)** – Indicates that a payroll batch (a group payment processing) includes a given timesheet. Essentially a grouping: e.g. PayrollBatch Sept1-15 \-\[:INCLUDES\]-\> T123, and also \-\> T124, T125, etc. This helps when verifying if all timesheets have been paid, or linking back from a payroll run to individual entries.

  * **(:ComplianceRule)-\[:APPLIES\_TO\]-\>(:ShootDay)** (or **\-\>(:Scene)** or **\-\>(:CrewRole)**) – Links a rule to the entity it constrains. The ontology gave examples: a union rule might apply to all scenes that go past a certain hour (so APPLIES\_TO those Scene nodes), or a safety rule applies to a specific shoot day (APPLIES\_TO that ShootDay), or a role (like all Stunt Performers must have a certain certification, which could be APPLIES\_TO a CrewRole node “Stunt Performer”). This is a flexible relationship used to mark what context a rule should be checked in. In practice, implementing this requires encoding the logic somewhere, but as a data relationship it flags where attention is needed. For example, if **Rule: Children under 16 cannot work past 10 PM** and we have a ShootDay that is scheduled to go till midnight with a child actor (Crew role “Child Actor”), we would link that rule to that ShootDay or even to that specific Scene, indicating a potential violation. Then automated checks can find that link and raise an alert.

  * **(:InsuranceDoc)** relationships: The ontology description implied needing to check if an insurance document is present for certain scenarios. We can model this as linking InsuranceDoc to the thing it covers. For example, *Medic Certificate for Stunt Scene* – we could link the InsuranceDoc node to the Scene or ShootDay that involves stunts. Perhaps **(:InsuranceDoc)-\[:COVERS\]-\>(:Scene)** (or ShootDay). Another approach: link InsuranceDoc to ComplianceRule (like *StuntInsuranceDoc* \-\[:SATISFIES\]-\> *“Medic on set” Rule*). The text example: “traverse from Scene to ComplianceRule to see if there’s a linked InsuranceDoc file in the File ontology” suggests they envision: Scene \-\> (via some relationship) ComplianceRule (like “StuntRequiresInsurance”) \-\> (via USES or REQUIRES) InsuranceDoc \-\> (via STORED\_AS) File. That’s a bit indirect. Simpler: we can directly link ComplianceRule to an InsuranceDoc: e.g., (ComplianceRule “MedicRequired”)-\[:HAS\_DOCUMENT\]-\>(InsuranceDoc “MedicCert\#123”). Then InsuranceDoc \-\[:STORED\_AS\]-\> File (the actual PDF of the certificate). In absence of a decided modeling, we note the likely need: to mark that certain required documents exist. **FIX (proposal):** For each compliance requirement that expects a document, introduce an InsuranceDoc node (or more generally a Document node) and link it with something like \[:REQUIRED\_DOC\] from the rule. Then when a document is provided, link the Project (or relevant entity) to that InsuranceDoc. E.g., Project X \-\[:HAS\_INSURANCE\]-\> InsuranceDoc\#123, and Rule “General Liability” \-\[:REQUIRED\_DOC\]-\> (InsuranceDoc type=GL). Checking compliance then becomes: for each Rule that APPLIES\_TO Project X (or its scenes/days), see if the Project (or scene/day) HAS\_INSURANCE of the required type. The exact modeling can vary, but the key point is the graph can represent both the requirement and the provided document, enabling an algorithm to validate compliance by traversal.

* **Explanation & Integration:** Currently, the Ops layer is **not yet implemented in the code** – it’s a blueprint for upcoming functionality. The system does have a notion of **Tasks** (there’s a `tasks` table and Task nodes), which are more like project to-do items (e.g., “Scout locations” or “Review budget”). Those are basic and not the financial/ops entities described above. None of Vendor/PO/Invoice/Timesheet data exists yet in the repository (no tables or sync handlers for them), so we label this entire section as **GAP** in implementation.

   **GAP:** No Supabase tables or Neo4j sync currently handle Vendors, POs, Invoices, Timesheets, etc.

   **FIX:** Introduce new tables and sync handlers for these operational entities, and establish the relationships to the existing graph. For instance, we could have a `vendors` table (id, name, contact) → sync to `Vendor` nodes; a `purchase_orders` table (id, project\_id, scene\_id nullable, crew\_role nullable, vendor\_id, amount, description, date) → sync to `PO` nodes and create relationships: to Project (maybe `BELONGS_TO_PROJECT` similar to files, to quickly scope all POs of a project), and if `scene_id` is set, create `(:PO)-[:FOR_SCENE]->(:Scene)`; if `crew_role` or `department` is set, either create a CrewRole node or use the role string to attach via FOR\_ROLE. Likewise an `invoices` table with vendor\_id, po\_id, etc., would sync to `Invoice` nodes and `FROM_VENDOR`, `PERTAINS_TO` relationships. Timesheets could be recorded in a `timesheets` table with crew\_id (foreign key to crew), project\_id, shoot\_day (date or an ID if shoot days listed), hours, etc. The sync would create a Timesheet node, link it to the Crew node (FOR\_CREW) and possibly to a ShootDay (we’d need a way to identify shoot days, which might be derived from date – if we have ShootDay nodes with date properties, we could match by date). A `payroll_batches` table could group timesheets by some batch/pay date, syncing to PayrollBatch node and INCLUDE relationships.

Once implemented, these additions will greatly enhance query capabilities. For example, the graph could answer: *“Show me all POs for scenes in the warehouse location, and whether their invoices are paid.”* The traversal would go: Location "Warehouse" \-\> Scenes set there \-\> POs for those scenes \-\> Invoices pertaining to those POs \-\> check invoice status property. Or: *“List any compliance rules that apply to tomorrow’s shoot (ShootDay 10\) that are not satisfied.”* This would involve: ShootDay10 \-\> applicable ComplianceRule(s) \-\> see if there’s a corresponding InsuranceDoc or other mitigation connected. Because the creative, scheduling, and ops data all meet in the graph, an automated assistant could reason across them (e.g., flag that *“Scene 5 is scheduled for Day 10 and involves a stunt, but no insurance doc is linked for stunts on Day 10”*, combining knowledge from all three ontology layers).

In summary, the Ops ontology ensures that every logistical piece of the production (money, people’s time, safety measures) is represented and tethered to the creative plan and the files. It turns the production from just a set of files and notes into a **fully connected knowledge graph** where, for instance, a single scene node can lead you to: the script pages (File ontology), the actors and props needed (Content ontology), the budget spent on it (PO/Invoice in Ops ontology), and the compliance checklist (Provenance/Compliance layer). This holistic view is powerful for both project management and AI reasoning about the production.

### 1.4 Provenance & Versioning (Audit Truth)

This layer provides a **temporal and provenance dimension** to the entire knowledge graph. Inspired by version control (Git) and event sourcing, it doesn’t introduce new domain entities but rather tracks changes, authorship, and history of the other entities. The goal is to ensure that every modification to the graph (be it a script change, a schedule update, or an automated annotation) is recorded as an immutable event. This allows us to query past states (“What did the schedule look like last week?”) and to understand why the data is the way it is (“Which user or process updated this scene’s shooting date, and what did it used to be?”). In high-stakes production data, this kind of audit trail is critical.

* **Node Types:**

  * **:`Commit`** – Represents a set of changes applied as one logical transaction, similar to a commit in Git. Properties might include `id` (a unique identifier, possibly a hash or UUID), `message` (commit message describing the changes), `author` (who made the change – could be a user id or system id), `timestamp` (when the commit was created), and possibly `branch` (the branch or environment the commit belongs to, e.g., “main” or “schedule-experiment”). A Commit node can have one or multiple parent commits (for merges or branching). This is analogous to a Git commit object. None of this is currently implemented in the codebase (the concept is described in design only – **GAP**). If implemented, every time the system processes an update (say, the user moves a file, or the AI generates a new breakdown result), it would create a Commit node capturing that event.

  * **:`Action`** – An atomic operation or tool invocation that occurred as part of a commit. If a Commit is a high-level grouping (like “Generated Call Sheet PDF”), Actions are the low-level steps and interactions within it. Properties: `tool` or `action_type` (e.g. “WeatherAPI.fetch” or “OCR.extractText”), `inputs` (maybe a JSON of input parameters), `outputs` (JSON of results or a reference), `status` (success/fail), and `timestamp` (when that action ran). In essence, each Action is a log of something the system did or a user did, which touched the data. For example, an Action could be “User edited Scene 5’s location via UI” or “AI assistant extracted 10 props from scene text.” In our current system, we don’t explicitly log such steps to the graph – **GAP**. However, we do have logs and the breakdown process internally notes some operations (the breakdown pipeline prints log messages for nodes like ledger merger, etc. and collects `sync_operations` in memory). Formalizing that into Action nodes is a to-do.

  * **:`EntityVersion`** – An immutable snapshot of a domain entity (Project, Scene, File, etc.) at a point in time. Properties: it needs to store the state of the entity's properties and the validity interval. One approach is a property set like `{entity_id: <ID of the entity>, label: <Label of the entity, e.g. "Scene">, props: <a map or JSON of all properties of the entity at that time>, valid_from: <timestamp>, valid_to: <timestamp or null if current>, tx_time: <commit timestamp when this version was recorded>}`. By storing the full property map, we have the exact data as of that version. Alternatively, we could store only the changed fields, but full snapshot is simpler for bitemporal queries. Each EntityVersion node is tied to the "real" Entity by a relationship (see below). *Implementation status:* not yet present – **GAP**. Currently, when the sync service upserts a node (e.g. a File), it just overwrites or creates the node in place, so historical values are lost. Introducing EntityVersion means changing that approach (instead of overwriting node properties directly, create new version nodes and link them, or maintain both current node and version records).

  * **:`EdgeFact`** – To properly branch, diff, and merge relationships, we represent each logical relationship as an immutable **:EdgeFact** node with a stable ID and bitemporal validity, rather than treating raw Neo4j relationships as the source of truth. Shape: `(:EdgeFact { id, edge_type, from_id, to_id, props: map, valid_from, valid_to, branch })`. Anchoring: `(from:Entity)-[:FACT_OUT {type: edge_type}]->(ef:EdgeFact)-[:FACT_IN]->(to:Entity)`. Provenance: `(:Commit)-[:UPDATES]->(:EdgeFact)`; actions `TOUCHED` it as with EntityVersion. Reads: the **current** relationship set is the union of all EdgeFacts with `valid_to = null` on the active branch. Writes create a **new EdgeFact** (and close the prior one's `valid_to`). We never mutate historical EdgeFacts. This lets us define conflict units per edge (id/type/from/to) and reason about "moves" (e.g., the same Scene re‑assigned to a different ShootDay) as precise fact changes.

* **Relationships:**

  * **(:Commit)-\[:INCLUDES\]-\>(:Action)** – A commit groups one or more actions. Every Action that was part of a commit will have an INCLUDES link from the Commit. For instance, a commit “AI Breakdown 2025-08-09 15:30” might include actions for `PARSE_SCRIPT`, `EXTRACT_SCENE_5`, `UPDATE_SCENE_NODE` etc. If a commit is just a simple user edit, it might include only a single Action (e.g. “User changed Scene location”). This hierarchy lets us attach multiple log entries to one high-level event (especially useful for automated multi-step processes).

  * **(:Action)-\[:TOUCHED\]-\>(:Entity)** – Each Action node would have one or more TOUCHED relationships to the domain nodes that it read or wrote. This provides fine-grained lineage. For example, an OCR Action might have TOUCHED \-\> (File node it read) and TOUCHED \-\> (Script node it wrote text into). A “fetch weather” Action might TOUCHED \-\> (ShootDay node) because it added a weather forecast property to that ShootDay. We could further qualify the nature of touch with properties like `mode: "read"| "write"` on the relationship, or use separate relationships like READ and MODIFIED. The design just states TOUCHED for any interaction. Using this, you can trace, for any given node, *which* actions and hence which commits affected it. E.g., *“Why does Scene 5 have location ‘Warehouse’?”* – traverse (Scene5)\<-\[:TOUCHED\]- (Action) \<-\[:INCLUDES\]- (Commit) to see the commit where that was set, and then inspect the commit’s author and message.

  * **(:Entity)-\[:HAS\_VERSION\]-\>(:EntityVersion)** – Links a domain entity to all its versions over time. Essentially, EntityVersion nodes hang off of their “master” entity. For example, a Scene node (which might just contain static identifiers and relationships) would have multiple HAS\_VERSION edges to SceneVersion nodes, each capturing the state at different times. We might also encode the current version via a separate relationship or property (or simply the one with `valid_to = null` is current). Because an EntityVersion already stores `entity_id` and label, one might question if we need a separate persistent entity node at all. Many event-sourced systems use a separate stable identity node (so that relationships from other parts of the graph can always point to the stable node and not break when versions update). We will assume the presence of stable Entity nodes with minimal properties (just an id and maybe current state pointer) that serve as anchors for relationships, while detailed properties reside in versions. (In some cases, we could use the latest version node as the one connected in the graph, but then updating relationships is tricky – so anchor node is cleaner).

  * **(:Commit)-\[:UPDATES\]-\>(:EntityVersion)** – Indicates that a commit resulted in a new version of that entity. In practice, we might attach this relationship to all EntityVersion nodes that were created by that commit. For instance, if commit C123 updated Scene5 and Scene7, and created new versions for each, we would have C123 \-\[:UPDATES\]-\> Scene5Version\#4, and C123 \-\[:UPDATES\]-\> Scene7Version\#2. This directly links the commit to the outcome states. In combination with the HAS\_VERSION chain, one can traverse: Project \-\> Scene5 \-\> (HAS\_VERSION) \-\> Scene5Version\#4 \<-\[:UPDATED\_BY\]- Commit C123 to see when and by whom that version was made. Optionally, we might have also (Commit)-\[:UPDATES\]-\> the *entity* itself (like to indicate which entities were touched at all), but since we have Action-\>Entity and Commit includes Actions, that might be redundant. The important part is connecting commit to new version nodes for easy lookup of “what new versions were introduced in this commit?”

  * **(Branch)-\[:HEAD\]-\>(Commit)** *(optional)* – If we implement branching, a Branch node (or we could use a special label on Commit for branch heads) would point to the latest commit on that branch. Branch nodes might simply have a name property (e.g., “main”, “schedule\_experiment”). In Git terms, this is like a ref. This allows multiple parallel universes of the graph (for example, an alternate schedule or a what-if scenario) to be maintained simultaneously. The text mentions the ability to “try an alternate production schedule on a separate branch” – we could do that by creating a branch, forking from current state, and then applying changes there (commits on that branch). Merging branches in a graph database context would be a complex operation (could involve copying subgraphs or toggling which version is active). This is advanced usage and not implemented yet. We note it as a design idea – none of our current code deals with branching.

* **Explanation:** The provenance layer essentially turns our graph into a **bitemporal ledger**. We track both:

  * **Transaction time** (when the data was recorded in the system) – via the Commit timestamp and the inherent ordering of commits (and perhaps the `tx_time` in EntityVersion).

  * **Valid time** (the time period the data is true in the real world) – via `valid_from` and `valid_to` on EntityVersion. For example, suppose on Aug 1 we scheduled Scene 5 for ShootDay Aug 9\. Then on Aug 5, we move Scene 5 to Aug 12\. In the graph, the Scene5 node would have had a relationship to ShootDay(Aug9) which was valid from Aug1 to Aug5, and then that relationship (or perhaps the ShootDay property on the scene’s version) is closed, and a new version of Scene5 with ShootDay \= Aug12 is created valid from Aug5 onward. The commit on Aug5 that made this change would have an audit trail: Commit message “Rescheduled scene 5 to Aug12”, author Bob, etc., and an Action “Update Scene Scheduling” that touches Scene5. The old version of Scene5 (with date Aug9) gets `valid_to = Aug5`. This way, if later asked “What was the schedule on Aug4?”, the system can recreate it by considering only versions whose valid\_from \<= Aug4 \< valid\_to. Essentially, we never delete or outright modify historical data; we end-date it and add new. Neo4j doesn’t natively enforce bitemporality, but our application logic would do so by how we write versions.

Because every change is recorded as an event (Commit with Actions), we achieve complete **traceability**. This is similar to how TerminusDB or Datomic maintain datoms, or how ledger databases record every state change. The mention *“both worlds co-exist: you can preserve the past and still update it going forward”* captures this: if we discover an error in past data, we don’t retroactively delete it – we can append a correction (perhaps on a branch or with a new valid time). For example, if a Scene’s name was entered incorrectly, we can create a new version with the correct name starting now, but keep the old name version in history (perhaps even mark it as corrected). The provenance layer can support queries like “show me the change history of Scene 5” (traverse Scene5 HAS\_VERSION \-\> list of versions, each with its commit info, diff the props) or “who last modified the budget file?” (traverse FileX \<-TOUCHED- Action \<-INCLUDES- Commit, get Commit.author).

**Current State:** At the moment, our system does not implement this layer. Data changes (like file or project updates) are applied directly and the previous state is lost except for what’s in Postgres WAL or backups. There is some groundwork: for instance, the `sync_events` outbox in Supabase acts as a log of changes at a high level (INSERT/UPDATE/DELETE operations for each table with timestamps). The sync worker uses that, but once it applies the change to Neo4j, it doesn’t keep the event. Also, our `models.entities.BaseEntity` has `created_at` and `updated_at` timestamps which give a basic history in the relational DB for each row (last updated time), but not a full version history. We also capture `synced_at` on nodes to know when it was last synced to Neo4j. To truly implement provenance in Neo4j, we need to augment the sync logic significantly.

**GAP:** No commit/action/version nodes exist yet; the Neo4j graph is currently storing only the latest state of each entity. There is no mechanism to query historical states or to see an audit trail of changes in Neo4j.

**FIX:** Implement an event-sourcing mechanism on top of the sync process. Concretely, when the sync worker (or any process) is about to mutate the graph, we do the following:

1. **Start a Commit:** Create a new `Commit` node with a generated ID (could be a combination of timestamp and a sequence or a UUID), set its `author` (if the change came from a user action, use that user’s ID; if from an automated tool, use a system ID like “sync\_worker”), set `message` (for automated events, a template message like “Sync update from table X”; for user, perhaps we pass in a description), and `created_at` timestamp. Also determine if it’s on the “main” branch or another (for now, all on main).

2. **For each entity changed** (could be multiple if the event is a batch): create an `Action` node. For database-driven events, an Action might correspond to a single table row change. For example, if a file’s metadata changed, we create Action like `{tool: "sync_worker.file_update", inputs: <file_id and new data>, timestamp: now}`. If it’s a user operation through the API, maybe `{tool: "user_interface", action: "edit_scene", inputs: {...}}`. Each Action gets linked to the Commit via INCLUDES.

3. **Record pre-change state:** Before applying the change, fetch the current properties of the affected node(s) from Neo4j (or from the event data, if the event includes previous values). Create an `EntityVersion` node representing the old state *if not already recorded*. We might need to record the old state to mark its valid\_to. Alternatively, if we always create versions for initial state too, then at creation time we had version1, and now we are making version2. In an event-sourced model, typically the new state is what we record. But we also need to update the old version’s `valid_to`. This implies we must find the current active version node of that entity and set its valid\_to to now (end it). In Neo4j, setting a property is a mutation – which we *could* allow since it’s just closing an interval (the historical record is still there). This operation itself might be logged as part of the Commit as well (maybe as a special Action or included in the same Action).

4. **Apply the new state:** Instead of overwriting the existing Entity node’s properties, we could do one of two things: (a) *Immutable entity nodes:* Don’t store changing props on the entity node at all, only on version nodes. Then the new version node gets created with `valid_from = now()` and open-ended `valid_to`. The Entity node remains as an identity container and maybe has a pointer to current version for convenience (or we derive current by querying for the version with `valid_to IS NULL`). (b) *Mutable entity with version history:* Update the Entity node’s properties to new values for quick access, but also create a new EntityVersion node with those same new values. This duplicates data but ensures both fast current reads and historical records. This approach might violate pure immutability but is pragmatic. Either way, we **create an EntityVersion node for the new state**. For example, if Scene5’s location changed, we make a SceneVersion node with props like `{scene_id:5, location:"Warehouse", ... , valid_from: Aug5 10:00, valid_to: null}`. Then link the Scene node to this new version via HAS\_VERSION. Also link the Commit to this EntityVersion via UPDATES. And the Action that triggered it would TOUCHED the Scene (and we could even have Action-\> old version, Action-\> new version if we want to be explicit, but likely Action-\>Entity covers it, and Commit-\>Version covers the outcome).

5. **Repeat for all changes in the commit**, then finalize the Commit node (set any final properties, maybe a hash of changes). If branching is in play, update branch HEAD pointers.

With the above in place, querying historical data becomes possible with Cypher. For instance, to get the state of all scenes as of August 4, you’d do `MATCH (s:Scene)-[:HAS_VERSION]->(v:SceneVersion) WHERE datetime("2025-08-04T00:00:00") >= v.valid_from AND datetime("2025-08-04T00:00:00") < v.valid_to RETURN s.id, v.props`. We can also get commit logs: `MATCH (c:Commit)-[:UPDATES]->(v:SceneVersion)-[:HAS_VERSION]-(s:Scene {id:5}) RETURN c.message, c.author, c.created_at` would list all commits that updated Scene5, with messages and authors. This is directly analogous to a `git log` for that scene. The **branching** capability means we could have alternate parallel Version chains. For example, if we want to propose a new schedule on a “experiment” branch, we’d copy or create new SceneVersion nodes under a different branch context. Perhaps the `Commit.branch` property or a Branch node association would tell us those versions belong to an alternate timeline. Merging branches would then involve reconciling those versions into the main branch (which could be done by creating a new commit on main with those changes).

While none of our current code uses this system, it is heavily influenced by known systems: TerminusDB (which uses a delta log and commits for JSON-LD graphs) and W3C **PROV-O** (Provenance Ontology), which provides a vocabulary for things like Activity (analogous to Action), Entity (our data entities and versions), Agent (users or systems), and relations like wasGeneratedBy (an entity version was generated by an action) and used (an action used an entity). In fact, our design aligns with PROV-O: Commit \~ Activity, Action \~ Activity step or just an Activity with sub-steps, EntityVersion \~ Entity (immutable state), and the relationships are akin to PROV “wasDerivedFrom”, “wasAttributedTo”, etc. Embracing a standard like PROV-O could make our audit trail interoperable, but even without that, the principles remain: **every change is an event, every state is preserved**.

In summary, the Provenance & Versioning layer turns the knowledge graph into a time-machine. We can ask the system "What was true at time T?" or "Why does this piece of data look like this?" and get answers backed by an immutable history. This guards against accidental data loss (we don't lose information by overwriting) and provides accountability (every change has an author and timestamp). It also enables advanced workflows like **branching**: for instance, planning a re-shoot schedule on a separate branch, comparing it with the original (two sets of ShootDay and Scene version nodes, perhaps) and then merging if approved. The design ensures that even after many changes, we can always reconstruct any past state or audit the sequence of events – crucial for productions where schedules and plans change frequently and decisions need to be justified with historical context.

#### **Provenance Write Path (Atomic, Idempotent).** All writes land via a single Cypher transaction that: (1) creates a Commit (id, author, ts, branch, message, dedupe_key), (2) for each mutated **entity**: closes the current `EntityVersion.valid_to`, creates a new version, and updates the live node, (3) for each mutated **temporal edge**: closes the current `EdgeFact.valid_to`, creates a new fact, and (re)materializes the live relationship, (4) creates `Action` nodes for every tool invocation and attaches `TOUCHED` to **both** old and new versions/facts, (5) writes a `commit->parent` link and updates `Branch->HEAD`, (6) enforces idempotency via a unique `Commit.dedupe_key`. The full transaction and schema are specified in §5 Provenance Write Path.

**AUDIT METRICS:** Commit and Action nodes double as an audit metric surface for observability. Until Actions ship, services emit equivalent counters and histograms; once present, the commit graph itself becomes a primary observability surface (e.g., commits/sec, failed actions by tool, agent attribution patterns).

**Integration Requirements:**
- **SDK Wrapper:** Create `GraphProvenance.write(tx, params)` wrapper that invokes the Provenance Write Path Cypher. Export from shared lib used by sync_worker, langgraph agents, and REST/gRPC endpoints.
- **Prohibit Direct Mutations:** Deprecate or route direct mutation helpers (e.g., `neo4j_client.upsert_node`) through the provenance writer. No exceptions.
- **Edge Key Patterns:** One-to-one edges use `edge_key = "${type}:${src_id}"`. Many-to-many include `dst_id` or domain key.
- **Idempotency Policy:** Writers MUST supply stable `dedupe_key`: Sync Worker uses Supabase outbox `event_id`, LangGraph agents use workflow `run_id` + step id, User API uses server-generated UUID.

* `:Commit { id, dedupe_key, author, ts, branch, message, applied }`
  * `dedupe_key` is **required** (e.g., Supabase outbox event id) and **unique** per commit.
  * `applied` guards conditional execution inside the txn.

{{ ... }}
  * One node per branch (e.g. `"main"`). HEAD is replaced atomically each commit.

* `:EntityVersion { vid, entity_label, entity_id, props, valid_from, valid_to, branch }`
  * Exactly **one** open version (`valid_to = null`) per `(entity_label, entity_id, branch)` (enforced by write logic).
  * Stable entity nodes (e.g. `:Scene`, `:File`) remain anchors; we still mirror "current" props onto them for fast reads.

* `:EdgeFact { fid, edge_key, type, src_label, src_id, dst_label, dst_id, props, valid_from, valid_to, branch }`
  * Temporal record for a relationship. `edge_key` identifies a logical edge timeline (e.g. `${type}:${src_id}->${dst_id}` or a domain key if many parallel edges exist).
  * Live relationship is (re)materialized each time to reflect the **current** fact.

* `:Action { id, tool, inputs, outputs, ts }`
  * `(:Commit)-[:INCLUDES]->(:Action)`
  * `(:Action)-[:TOUCHED {mode: 'read'|'write'}]->(:EntityVersion|:EdgeFact)` to both **old** and **new** objects.

**Neo4j constraints / indexes (migration block):**

```cypher
// Commit
CREATE CONSTRAINT commit_id_unique    IF NOT EXISTS FOR (c:Commit) REQUIRE c.id IS UNIQUE;
CREATE CONSTRAINT commit_dedupe_unique IF NOT EXISTS FOR (c:Commit) REQUIRE c.dedupe_key IS UNIQUE;

// Branch
CREATE CONSTRAINT branch_name_unique  IF NOT EXISTS FOR (b:Branch) REQUIRE b.name IS UNIQUE;

// Versions & Facts (lookup performance)
CREATE INDEX entity_version_lookup IF NOT EXISTS FOR (v:EntityVersion) ON (v.entity_label, v.entity_id, v.branch, v.valid_to);
CREATE INDEX edge_fact_lookup     IF NOT EXISTS FOR (f:EdgeFact)     ON (f.edge_key, f.branch, f.valid_to);

// Optional but helpful
CREATE CONSTRAINT ev_vid_unique   IF NOT EXISTS FOR (v:EntityVersion) REQUIRE v.vid IS UNIQUE;
CREATE CONSTRAINT ef_fid_unique   IF NOT EXISTS FOR (f:EdgeFact)     REQUIRE f.fid IS UNIQUE;
CREATE CONSTRAINT action_id_unique IF NOT EXISTS FOR (a:Action)      REQUIRE a.id IS UNIQUE;
```

> **Note:** Unique per‑label **id** constraints you already have (e.g., `:File.id`) remain; they're used to locate anchors quickly.

---

### 1.5 Temporal Edges (EdgeFacts)

Some truths live on edges and change over time—e.g., `(:ShootDay)-[:COVERS_SCENE]->(:Scene)`, crew assignments, and file↔slot classification. To model history, diffs, rollbacks, and branches correctly, we **reify each such relationship as a versioned node** and continue to maintain a **current materialized relationship** for low‑latency reads.

**Core pattern**

* **Node:** `:EdgeFact`
  **Properties:**

  * `id` (UUID)
  * `type` (string; e.g. `"COVERS_SCENE"`, `"ASSIGNED_TO"`, `"APPLIES_RULE"`)
  * `from_id` (string/UUID of source entity)
  * `to_id` (string/UUID of target entity)
  * `props` (map; e.g., role, weight, notes, confidence, rule_id)
  * `branch` (string; default `"main"`)
  * `valid_from` (datetime)
  * `valid_to` (datetime or null if current)
  * `tx_time` (datetime; commit time)

* **Links:**

  * `(:Entity)-[:HAS_EDGE]->(:EdgeFact)` (anchor to either endpoint; we keep both for convenience)
  * `(:EdgeFact)-[:FROM]->(:Entity)` and `(:EdgeFact)-[:TO]->(:Entity)`

* **Materialized "current" rels:** For each EdgeFact type we maintain a same‑named Neo4j relationship between the actual entities (e.g., `(:ShootDay)-[:COVERS_SCENE]->(:Scene)`) **reflecting only the latest open‑interval EdgeFact** for that `(type, from_id, to_id, branch)`. These convenience rels are created/updated **in the same write transaction** that opens/closes EdgeFacts.

**Writes (single transaction)**

1. **Close** any open EdgeFact for this `(type, from, to, branch)` by setting `valid_to = now()`.
2. **Create** the new EdgeFact with `valid_from = now(), valid_to = null`.
3. **Update convenience rels**: create/refresh the domain relationship for the new current fact and remove any convenience rels that are no longer current.
4. **Provenance**: link the new EdgeFact to the `:Commit` via `(:Commit)-[:UPDATES]->(:EdgeFact)` and log an `:Action` that `:TOUCHED` both endpoint entities and the EdgeFact.

**Reads**

* **Current state:** use the materialized `[:TYPE]` relationships for speed.
* **Historical state:** query `:EdgeFact` intervals with `valid_from <= $t < coalesce(valid_to, datetime('9999-12-31'))`.

This pattern keeps **clean bitemporality** for edge truths while preserving **fast "now" queries**. It complements the node‑level `:EntityVersion` already described in 1.4.

---
